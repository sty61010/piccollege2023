{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the model architecture using a self-attention transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch import Tensor\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the diffusion model\n",
        "class DiffusionModel(nn.Module):\n",
        "    def __init__(self, n_steps, n_heads, n_dims, n_hidden, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_steps = n_steps\n",
        "        self.n_heads = n_heads\n",
        "        self.n_dims = n_dims\n",
        "        self.n_hidden = n_hidden\n",
        "\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(n_dims, n_heads, n_hidden) \n",
        "            for _ in range(n_steps)\n",
        "        ])\n",
        "\n",
        "        self.to_params = nn.Linear(n_dims, 2 * n_dims)\n",
        "        self.to_output = nn.Linear(n_dims, output_dim)\n",
        "\n",
        "    def sample_noise(self, batch_size, device):\n",
        "        return torch.randn(batch_size, self.n_dims, device=device)\n",
        "\n",
        "    def forward(self, x, timesteps_left):\n",
        "        # apply the diffusion process\n",
        "        for i in reversed(range(self.n_steps)):\n",
        "            # sample noise for this step\n",
        "            noise = self.sample_noise(x.shape[0], x.device)\n",
        "\n",
        "            # get the parameters for this step\n",
        "            params = self.to_params(x)\n",
        "            mean, log_std = params.chunk(2, dim=-1)\n",
        "\n",
        "            # calculate the new state\n",
        "            std = torch.exp(log_std)\n",
        "            state = (x - mean) / std\n",
        "            state = state + noise * std\n",
        "            state = self.transformer_layers[i](state.transpose(0, 1), None).transpose(0, 1)\n",
        "\n",
        "            # update x with the new state\n",
        "            x = mean + state * std\n",
        "\n",
        "            # apply the schedule\n",
        "            x = x * (1 - timesteps_left[i]) + noise * timesteps_left[i].sqrt()\n",
        "\n",
        "        # map the final state to RGB values\n",
        "        output = self.to_output(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 1000])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 3, got 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/home/cytseng/piccollege/PicCollege_HW3_duffision.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcml0.csie.ntu.edu.tw/home/cytseng/piccollege/PicCollege_HW3_duffision.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m iteration \u001b[39m=\u001b[39m tqdm(\u001b[39mrange\u001b[39m(num_epochs))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcml0.csie.ntu.edu.tw/home/cytseng/piccollege/PicCollege_HW3_duffision.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m iteration:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcml0.csie.ntu.edu.tw/home/cytseng/piccollege/PicCollege_HW3_duffision.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train_diffusion(model, optimizer, criterion, dataloader, device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcml0.csie.ntu.edu.tw/home/cytseng/piccollege/PicCollege_HW3_duffision.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m     iteration\u001b[39m.\u001b[39mset_description(\u001b[39m'\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m], Train Loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, num_epochs, train_loss))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcml0.csie.ntu.edu.tw/home/cytseng/piccollege/PicCollege_HW3_duffision.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m \u001b[39m# Generate some samples from the model\u001b[39;00m\n",
            "\u001b[1;32m/home/cytseng/piccollege/PicCollege_HW3_duffision.ipynb Cell 4\u001b[0m in \u001b[0;36mtrain_diffusion\u001b[0;34m(model, optimizer, criterion, dataloader, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcml0.csie.ntu.edu.tw/home/cytseng/piccollege/PicCollege_HW3_duffision.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mprint\u001b[39m(timesteps_left\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcml0.csie.ntu.edu.tw/home/cytseng/piccollege/PicCollege_HW3_duffision.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# generate the RGB values using the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcml0.csie.ntu.edu.tw/home/cytseng/piccollege/PicCollege_HW3_duffision.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(noise, timesteps_left)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcml0.csie.ntu.edu.tw/home/cytseng/piccollege/PicCollege_HW3_duffision.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, batch)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcml0.csie.ntu.edu.tw/home/cytseng/piccollege/PicCollege_HW3_duffision.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
            "File \u001b[0;32m~/miniconda3/envs/mm_petr/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32m/home/cytseng/piccollege/PicCollege_HW3_duffision.ipynb Cell 4\u001b[0m in \u001b[0;36mDiffusionModel.forward\u001b[0;34m(self, x, timesteps_left)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcml0.csie.ntu.edu.tw/home/cytseng/piccollege/PicCollege_HW3_duffision.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m state \u001b[39m=\u001b[39m (x \u001b[39m-\u001b[39m mean) \u001b[39m/\u001b[39m std\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcml0.csie.ntu.edu.tw/home/cytseng/piccollege/PicCollege_HW3_duffision.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m state \u001b[39m=\u001b[39m state \u001b[39m+\u001b[39m noise \u001b[39m*\u001b[39m std\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcml0.csie.ntu.edu.tw/home/cytseng/piccollege/PicCollege_HW3_duffision.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_layers[i](state\u001b[39m.\u001b[39;49mtranspose(\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m), \u001b[39mNone\u001b[39;49;00m)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcml0.csie.ntu.edu.tw/home/cytseng/piccollege/PicCollege_HW3_duffision.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# update x with the new state\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcml0.csie.ntu.edu.tw/home/cytseng/piccollege/PicCollege_HW3_duffision.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m x \u001b[39m=\u001b[39m mean \u001b[39m+\u001b[39m state \u001b[39m*\u001b[39m std\n",
            "File \u001b[0;32m~/miniconda3/envs/mm_petr/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/mm_petr/lib/python3.8/site-packages/torch/nn/modules/transformer.py:320\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src: Tensor, src_mask: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, src_key_padding_mask: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    310\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Pass the input through the encoder layer.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \n\u001b[1;32m    312\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39m        see the docs in Transformer class.\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     src2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(src, src, src, attn_mask\u001b[39m=\u001b[39;49msrc_mask,\n\u001b[1;32m    321\u001b[0m                           key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    322\u001b[0m     src \u001b[39m=\u001b[39m src \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(src2)\n\u001b[1;32m    323\u001b[0m     src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(src)\n",
            "File \u001b[0;32m~/miniconda3/envs/mm_petr/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/mm_petr/lib/python3.8/site-packages/torch/nn/modules/activation.py:1031\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1021\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1022\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1029\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight)\n\u001b[1;32m   1030\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1031\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1032\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1033\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1034\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1035\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1036\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1037\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1038\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask)\n\u001b[1;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first:\n\u001b[1;32m   1040\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
            "File \u001b[0;32m~/miniconda3/envs/mm_petr/lib/python3.8/site-packages/torch/nn/functional.py:4948\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4919\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   4920\u001b[0m         multi_head_attention_forward,\n\u001b[1;32m   4921\u001b[0m         tens_ops,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4944\u001b[0m         static_v\u001b[39m=\u001b[39mstatic_v,\n\u001b[1;32m   4945\u001b[0m     )\n\u001b[1;32m   4947\u001b[0m \u001b[39m# set up shape vars\u001b[39;00m\n\u001b[0;32m-> 4948\u001b[0m tgt_len, bsz, embed_dim \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39mshape\n\u001b[1;32m   4949\u001b[0m src_len, _, _ \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39mshape\n\u001b[1;32m   4950\u001b[0m \u001b[39massert\u001b[39;00m embed_dim \u001b[39m==\u001b[39m embed_dim_to_check, \\\n\u001b[1;32m   4951\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwas expecting embedding dimension of \u001b[39m\u001b[39m{\u001b[39;00membed_dim_to_check\u001b[39m}\u001b[39;00m\u001b[39m, but got \u001b[39m\u001b[39m{\u001b[39;00membed_dim\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ],
      "source": [
        "class ColoredPiDataset(Dataset):\n",
        "    def __init__(self, image_path, xs_path, ys_path):\n",
        "        self.xs = np.load(xs_path)\n",
        "        self.ys = np.load(ys_path)\n",
        "        self.image_array = np.array(Image.open(image_path))\n",
        "        self.rgb_values = self.image_array[self.xs, self.ys]\n",
        "        \n",
        "        # Normalize xy values to be between 0 and 1\n",
        "        self.xs, self.ys = self.xs / 299.0, self.ys / 299.0\n",
        "\n",
        "        # Normalize rgb values to be between 0 and 1\n",
        "        self.rgb_values = self.rgb_values / 255.0\n",
        "        \n",
        "        # # Normalize rgb values to be between -1 and 1\n",
        "        # self.rgb_values = (self.rgb_values / 127.5) - 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.xs)\n",
        "        # return 30000\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= 5000:\n",
        "            return torch.zeros((5)).to(torch.float32)\n",
        "        return torch.tensor([self.xs[idx], self.ys[idx], self.rgb_values[idx][0], self.rgb_values[idx][1], self.rgb_values[idx][2]]).to(torch.float32)\n",
        "\n",
        "# Define training function\n",
        "def train_diffusion(model, optimizer, criterion, dataloader, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in dataloader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        noise = torch.randn(batch.shape[0], 128).to(device)\n",
        "        \n",
        "        # generate the timesteps_left schedule\n",
        "        timesteps_left = torch.linspace(0, 1, model.n_steps, device=device)\n",
        "        timesteps_left = timesteps_left.expand(batch_size, -1)\n",
        "        print(timesteps_left.shape)\n",
        "        # generate the RGB values using the model\n",
        "        outputs = model(noise, timesteps_left)\n",
        "\n",
        "        loss = criterion(outputs, batch)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * batch.size(0)\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    return epoch_loss\n",
        "\n",
        "# Set up device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define hyperparameters\n",
        "input_dim = 5 # XYRGB values\n",
        "output_dim = 5 # XYRGB values\n",
        "hidden_dim = 128\n",
        "latent_dim = 16\n",
        "num_layers = 2\n",
        "num_heads = 4\n",
        "dropout = 0.1\n",
        "\n",
        "batch_size = 128\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 10\n",
        "num_samples = 500\n",
        "\n",
        "# Load the dataset\n",
        "dataset = ColoredPiDataset('sparse_pi_colored.jpg', 'pi_xs.npy', 'pi_ys.npy')\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "# Initialize model, optimizer, and loss function\n",
        "model = DiffusionModel(n_steps=1000, n_heads=4, n_dims=128, n_hidden=512, output_dim=output_dim).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Train model\n",
        "iteration = tqdm(range(num_epochs))\n",
        "for epoch in iteration:\n",
        "    train_loss = train_diffusion(model, optimizer, criterion, dataloader, device)\n",
        "    iteration.set_description('Epoch [{}/{}], Train Loss: {:.4f}'.format(epoch+1, num_epochs, train_loss))\n",
        "    \n",
        "# Generate some samples from the model\n",
        "generated_image = np.zeros(dataset.image_array.shape)\n",
        "\n",
        "# xy  = np.zeros((num_samples*batch_size, 2))\n",
        "# rgb = np.zeros((num_samples*batch_size, 3))\n",
        "# sample_iter = tqdm(range(num_samples))\n",
        "# for sample_idx in sample_iter:\n",
        "\n",
        "xy  = np.zeros((len(dataloader)*batch_size, 2))\n",
        "rgb = np.zeros((len(dataloader)*batch_size, 3))\n",
        "for sample_idx, batch in enumerate(dataloader):\n",
        "    with torch.no_grad():\n",
        "        # samples = model(torch.randn(batch_size, 5).to(device))\n",
        "        # samples, _, _ = model(torch.randn(batch_size, 5).to(device))\n",
        "        samples, _, _ = model(batch.to(device))\n",
        "        # samples = model.decode(torch.randn(batch_size, latent_dim).to(device))\n",
        "\n",
        "        # Denomarlizing samples\n",
        "        # samples[:, :2] = (samples[:, :2] + 1) * 149.5\n",
        "        samples[:, :2] = (samples[:, :2]) * 299\n",
        "        \n",
        "        # Denomarlizing samples\n",
        "        # samples[:, 2:] = (samples[:, 2:] + 1) * 127.5\n",
        "        samples[:, 2:] = (samples[:, 2:]) * 255\n",
        "        \n",
        "        xy[sample_idx*batch_size:(sample_idx+1)*batch_size, :] = samples[:, :2].cpu().numpy()\n",
        "        rgb[sample_idx*batch_size:(sample_idx+1)*batch_size, :] = samples[:, 2:].cpu().numpy()\n",
        "\n",
        "        samples = samples.cpu().numpy().astype(np.uint8)\n",
        "        for i in range(batch_size):\n",
        "            x, y, r, g, b = samples[i]\n",
        "            generated_image[x, y] = [r, g, b]\n",
        "            \n",
        "print(f'xy mean: {np.mean(xy)}, xy std: {np.std(xy)}, xy max: {np.max(xy)}, xy min: {np.min(xy)}')\n",
        "print(f'rgb mean: {np.mean(rgb)}, rgb std: {np.std(rgb)}, rgb max: {np.max(rgb)}, rgb min: {np.min(rgb)}')\n",
        "print(f'Error: {np.mean(np.abs(generated_image - dataset.image_array))}')\n",
        "\n",
        "# Save the output image\n",
        "# Image.fromarray(generated_image).save('generated_pi_colored.jpg')\n",
        "plt.imshow(generated_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "mm_petr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
